{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the unofficial implementation of paper Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models\n",
    "https://arxiv.org/abs/2305.04441\n",
    "\n",
    "Use ddim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First download the Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "import torch\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your own image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_image(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "\n",
    "    img_tensor = torch.from_numpy(img).float() / 127.5 - 1\n",
    "    img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device_0)\n",
    "    return img_tensor\n",
    "\n",
    "img_path = \"your_image_path\"\n",
    "image = load_image(img_path)\n",
    "x_0 = (ldm_stable.vae.encode(image).latent_dist.mode() * 0.18215).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDIM Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "ldm_stable.scheduler = scheduler\n",
    "ldm_stable.scheduler.set_timesteps(50)\n",
    "\n",
    "def encode_text(model, prompts):\n",
    "    text_input = model.tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=model.tokenizer.model_max_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        text_encoding = model.text_encoder(text_input.input_ids.to(model.device))[0]\n",
    "    return text_encoding\n",
    "\n",
    "def next_step(model, model_output: Union[torch.FloatTensor, np.ndarray], timestep: int, sample: Union[torch.FloatTensor, np.ndarray]):\n",
    "    timestep, next_timestep = min(timestep - model.scheduler.config.num_train_timesteps // model.scheduler.num_inference_steps, 999), timestep\n",
    "    alpha_prod_t = model.scheduler.alphas_cumprod[timestep] if timestep >= 0 else model.scheduler.final_alpha_cumprod\n",
    "    alpha_prod_t_next = model.scheduler.alphas_cumprod[next_timestep]\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    next_original_sample = (sample - beta_prod_t ** 0.5 * model_output) / alpha_prod_t ** 0.5\n",
    "    next_sample_direction = (1 - alpha_prod_t_next) ** 0.5 * model_output\n",
    "    next_sample = alpha_prod_t_next ** 0.5 * next_original_sample + next_sample_direction\n",
    "    return next_sample\n",
    "\n",
    "def get_noise_pred(model, latent, t, context, cfg_scale):\n",
    "    latents_input = torch.cat([latent] * 2)\n",
    "    noise_pred = model.unet(latents_input, t, encoder_hidden_states=context)[\"sample\"]\n",
    "    noise_pred_uncond, noise_prediction_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + cfg_scale * (noise_prediction_text - noise_pred_uncond)\n",
    "    # latents = next_step(model, noise_pred, t, latent)\n",
    "    return noise_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def ddim_inversion(model, w0, cfg_scale):\n",
    "    # uncond_embeddings, cond_embeddings = self.context.chunk(2)\n",
    "    # all_latent = [latent]\n",
    "    uncond_embedding = encode_text(model, \"\")\n",
    "    context = torch.cat([uncond_embedding, uncond_embedding])\n",
    "    latent = w0.clone().detach()\n",
    "    all_latent = []\n",
    "    for i in tqdm(range(model.scheduler.num_inference_steps)):\n",
    "        t = model.scheduler.timesteps[len(model.scheduler.timesteps) - i - 1]\n",
    "        noise_pred = get_noise_pred(model, latent, t, context, cfg_scale)\n",
    "        latent = next_step(model, noise_pred, t, latent)\n",
    "        all_latent.append((t, latent))\n",
    "    return all_latent\n",
    "\n",
    "xts = ddim_inversion(ldm_stable, x_0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pormpt Tuning process\n",
    "(Don't forget to give the target prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def ddim_forward(model, noise, t, latent):\n",
    "    latent = model.scheduler.step(noise, t, latent)[\"prev_sample\"]\n",
    "    return latent\n",
    "\n",
    "def prompt_tuning(model, xts, target_prompt, cfg_scale, beta, N):\n",
    "    c_list = []\n",
    "    target_embedding = encode_text(model, target_prompt)\n",
    "    c_T = torch.concat([target_embedding, target_embedding])\n",
    "    c_list.append(c_T)\n",
    "    opt = AdamW([c_T])\n",
    "    z_prd_t = xts[-1][1]\n",
    "    c_T.requires_grad_(True)\n",
    "    for i in range(model.scheduler.num_inference_steps - 1):\n",
    "        t = model.scheduler.timesteps[i]\n",
    "        z_t_minus_1 = xts[len(xts) - i - 2][1]\n",
    "        z_t_minus_1.requires_grad_(True)\n",
    "        for j in range(0, N):\n",
    "            print(f'iter {j}')\n",
    "            noise_prd = get_noise_pred(model, z_prd_t, t, c_T, cfg_scale)\n",
    "            z_prd_t_minus_1 = ddim_forward(model, noise_prd, t, z_prd_t)\n",
    "            z_prd_t_minus_1.requires_grad_(True)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            differnce = torch.norm(z_prd_t_minus_1 - z_t_minus_1, p=2) ** 2 \n",
    "            differnce.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            z_t_minus_1 = z_t_minus_1.detach()\n",
    "            z_prd_t_minus_1 = z_prd_t_minus_1.detach()\n",
    "        print(f'done')\n",
    "\n",
    "        c_list.append(c_T.clone())\n",
    "        \n",
    "        noise_prd_again = get_noise_pred(model, z_prd_t, t, c_T, cfg_scale)\n",
    "        z_prd_t_minus_1 = ddim_forward(model, noise_prd, t, z_prd_t)\n",
    "        z_prd_t = z_prd_t_minus_1.detach()\n",
    "    \n",
    "    return c_list\n",
    "\n",
    "target_prompt = \"your own prompt\"\n",
    "beta = 0.1\n",
    "c_list = prompt_tuning(ldm_stable, xts, target_prompt, cfg_scale=5, beta=beta, N=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def editing(model, xts, c_list, target_prompt, eta):\n",
    "    z_t = xts[-1][1]\n",
    "    target_embedding = encode_text(model, target_prompt)\n",
    "    c_target = torch.concat([target_embedding, target_embedding])\n",
    "    for i in range(model.scheduler.num_inference_steps):\n",
    "        t = model.scheduler.timesteps[i]\n",
    "        c_t = c_list[i]\n",
    "        c_t = (1 - eta) * c_t + eta * c_target\n",
    "        noise_prd = get_noise_pred(model, z_t, t, c_t, cfg_scale=1)\n",
    "        z_t = ddim_forward(model, noise_prd, t, z_t)\n",
    "    return z_t\n",
    "\n",
    "z_0 = editing(ldm_stable, xts, c_list, target_prompt, eta=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode the latent and save the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw ,ImageFont\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def tensor_to_pil(tensor_imgs):\n",
    "    if type(tensor_imgs) == list:\n",
    "        tensor_imgs = torch.cat(tensor_imgs)\n",
    "    tensor_imgs = (tensor_imgs / 2 + 0.5).clamp(0, 1)\n",
    "    to_pil = T.ToPILImage()\n",
    "    pil_imgs = [to_pil(img) for img in tensor_imgs]    \n",
    "    return pil_imgs\n",
    "\n",
    "def add_margin(pil_img, top = 0, right = 0, bottom = 0, \n",
    "                    left = 0, color = (255,255,255)):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right + left\n",
    "    new_height = height + top + bottom\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
    "    \n",
    "    result.paste(pil_img, (left, top))\n",
    "    return result\n",
    "\n",
    "def image_grid(imgs, rows = 1, cols = None, \n",
    "                    size = None,\n",
    "                   titles = None, text_pos = (0, 0)):\n",
    "    if type(imgs) == list and type(imgs[0]) == torch.Tensor:\n",
    "        imgs = torch.cat(imgs)\n",
    "    if type(imgs) == torch.Tensor:\n",
    "        imgs = tensor_to_pil(imgs)\n",
    "        \n",
    "    if not size is None:\n",
    "        imgs = [img.resize((size,size)) for img in imgs]\n",
    "    if cols is None:\n",
    "        cols = len(imgs)\n",
    "    assert len(imgs) >= rows*cols\n",
    "    \n",
    "    top=20\n",
    "    w, h = imgs[0].size\n",
    "    delta = 0\n",
    "    if len(imgs)> 1 and not imgs[1].size[1] == h:\n",
    "        delta = top\n",
    "        h = imgs[1].size[1]\n",
    "    if not titles is  None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", \n",
    "                                    size = 20, encoding=\"unic\")\n",
    "        h = top + h \n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h+delta))    \n",
    "    for i, img in enumerate(imgs):\n",
    "        \n",
    "        if not titles is  None:\n",
    "            img = add_margin(img, top = top, bottom = 0,left=0)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            draw.text(text_pos, titles[i],(0,0,0), \n",
    "            font = font)\n",
    "        if not delta == 0 and i > 0:\n",
    "           grid.paste(img, box=(i%cols*w, i//cols*h+delta))\n",
    "        else:\n",
    "            grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "        \n",
    "    return grid    \n",
    "\n",
    "\n",
    "z_decode = ldm_stable.vae.decode(1 / 0.18215 * z_0).sample\n",
    "if z_decode.dim() < 4:\n",
    "    z_decode = z_decode[None, :, :, :]\n",
    "img = image_grid(z_decode)\n",
    "save_path = \"your save path\"\n",
    "img.save(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
